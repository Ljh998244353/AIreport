# 轻量化大模型移动端部署研究 - 完整执行指南

## 项目概述

本项目实现从模型选择、高效微调、量化转换到移动端部署的一体化流程。使用 Qwen2.5-0.5B-Instruct 模型，通过 LoRA + QLoRA 进行微调，并转换为 GGUF 格式进行量化部署。

**环境要求：**
- WSL2 Ubuntu 24.04
- Python 3.8+
- CUDA 11.8+ (用于GPU训练)
- 至少 8GB 显存 (使用QLoRA可降低到4GB)
- 至少 20GB 磁盘空间

---

## 阶段一：模型选择与高效微调

### 步骤1：环境配置

#### 1.1 激活虚拟环境

```bash
# 进入项目目录
cd /home/ljh/AI

# 激活虚拟环境（如果已创建）
source venv/bin/activate

# 如果未创建虚拟环境，先创建：
# python -m venv venv
# source venv/bin/activate
```

#### 1.2 安装依赖包

```bash
# 使用清华大学镜像源（适合中国大陆）
pip install -r requirements.txt -i https://pypi.tuna.tsinghua.edu.cn/simple

# 如果bitsandbytes安装失败，可以尝试：
# pip install bitsandbytes -i https://pypi.tuna.tsinghua.edu.cn/simple
# 或者跳过bitsandbytes，使用标准LoRA（不使用4-bit量化）
```

**重要提示：**
- `bitsandbytes` 在WSL2中可能需要特殊配置，如果安装失败，可以修改训练脚本使用标准LoRA
- 确保PyTorch版本与CUDA版本匹配

#### 1.3 配置HuggingFace镜像（重要！）

```bash
# 设置环境变量（在 ~/.bashrc 或 ~/.zshrc 中添加）
export HF_ENDPOINT=https://hf-mirror.com

# 使环境变量生效
source ~/.bashrc  # 或 source ~/.zshrc

# 或者在当前终端会话中临时设置
export HF_ENDPOINT=https://hf-mirror.com
```

#### 1.4 验证GPU环境

```bash
# 检查CUDA是否可用
python -c "import torch; print(f'CUDA available: {torch.cuda.is_available()}'); print(f'CUDA version: {torch.version.cuda}'); print(f'GPU count: {torch.cuda.device_count()}')"

# 如果CUDA不可用，检查NVIDIA驱动
nvidia-smi
```

---

### 步骤2：数据准备

#### 2.1 下载数据集

运行优化后的数据下载脚本（已包含多种备用方案）：

```bash
python stage1_finetune/download_data.py
```

**脚本会自动尝试以下方案：**

1. **方案1：HuggingFace镜像站** (`hf-mirror.com`)
   - 自动从镜像站下载 ChnSentiCorp 数据集

2. **方案2：ModelScope（阿里云）**
   - 如果方案1失败，自动尝试从ModelScope下载
   - 会自动安装 `modelscope` 库（如果未安装）

3. **方案3：示例数据**
   - 如果前两个方案都失败，会创建示例数据集用于测试
   - 包含200条训练样本、20条验证样本、20条测试样本

**如果自动下载失败，可以手动下载：**

```bash
# 方法A: 使用ModelScope命令行工具
pip install modelscope -i https://pypi.tuna.tsinghua.edu.cn/simple
python -c "from modelscope import MsDataset; ds = MsDataset.load('seamew/ChnSentiCorp'); print(ds)"

方法B: 从GitHub/Gitee手动下载
访问: https://gitee.com/mirrors/ChineseNlpCorpus
下载 ChnSentiCorp 数据集文件到 data/chn_senti_corp/ 目录
```

**数据保存位置：**
- `data/chn_senti_corp/train.json` - 训练集
- `data/chn_senti_corp/validation.json` - 验证集
- `data/chn_senti_corp/test.json` - 测试集

#### 2.2 数据预处理

运行预处理脚本，将数据转换为模型训练格式：

```bash
python stage1_finetune/preprocess_data.py
```

**预处理脚本会：**
- 清洗数据（去除空值、重复项）
- 将数据转换为Qwen2.5格式的提示模板
- 划分训练集/验证集/测试集（8:1:1）
- 保存处理后的数据到 `data/processed/`

**输出文件：**
- `data/processed/train.json` - 训练集（已格式化）
- `data/processed/val.json` - 验证集（已格式化）
- `data/processed/test.json` - 测试集（已格式化）

**提示模板格式示例：**
```
<|im_start|>system
你是一个情感分析助手，需要判断文本的情感倾向。<|im_end|>
<|im_start|>user
请分析以下文本的情感倾向（正面/负面）：
这部电影很好看<|im_end|>
<|im_start|>assistant
正面<|im_end|>
```

---

### 步骤3：模型微调

#### 3.1 检查训练配置

查看并确认 `config/training_config.yaml` 中的配置：

```yaml
# 关键配置项：
model:
  name: "Qwen/Qwen2.5-0.5B-Instruct"
  use_4bit: true  # 使用QLoRA（4-bit量化）

lora:
  r: 8                    # LoRA秩（可减小以降低显存）
  lora_alpha: 16
  lora_dropout: 0.05

training:
  per_device_train_batch_size: 4  # 如果显存不足，可减小到2或1
  gradient_accumulation_steps: 4  # 相应增加以保持有效批大小
  num_train_epochs: 3
  learning_rate: 2.0e-4
```

**显存优化建议：**
- 如果显存不足（< 4GB），可以：
  - 将 `per_device_train_batch_size` 改为 1
  - 将 `gradient_accumulation_steps` 改为 8
  - 将 LoRA 的 `r` 参数从 8 改为 4
  - 将 `use_4bit` 设为 `true`（使用QLoRA）

#### 3.2 开始训练

```bash
python stage1_finetune/train_lora.py --config config/training_config.yaml
```

**训练过程说明：**

1. **模型下载**：首次运行会自动从镜像站下载 Qwen2.5-0.5B-Instruct 模型（约1GB）
   - 下载位置：`./models/cache/`
   - 如果下载失败，检查 `HF_ENDPOINT` 环境变量

2. **模型加载**：
   - 使用4-bit量化加载模型（QLoRA）
   - 添加LoRA适配器（仅增加数百万参数）

3. **训练监控**：
   - 训练损失（train_loss）
   - 验证损失（eval_loss）
   - 学习率变化
   - 训练进度条

4. **检查点保存**：
   - 每500步保存一次检查点
   - 保存在 `outputs/checkpoints/checkpoint-XXX/`
   - 最终模型保存在 `outputs/checkpoints/final/`

**训练时间估算：**
- GPU (RTX 3060 12GB): 约30-60分钟
- GPU (RTX 4090): 约15-30分钟
- CPU: 可能需要数小时（不推荐）

#### 3.3 训练中断恢复

如果训练中断，重新运行相同命令即可从最新检查点继续：

```bash
# 训练脚本会自动检测并加载最新检查点
python stage1_finetune/train_lora.py --config config/training_config.yaml
```

---

### 步骤4：模型评估

训练完成后，评估模型性能：

```bash
python stage1_finetune/evaluate.py --model_path outputs/checkpoints/final
```

**评估指标：**
- 准确率 (Accuracy)
- 精确率 (Precision)
- 召回率 (Recall)
- F1分数
- 混淆矩阵

**输出文件：**
- `outputs/checkpoints/final/adapter_config.json` - LoRA配置
- `outputs/checkpoints/final/adapter_model.bin` - LoRA权重文件（仅几MB）
- `outputs/checkpoints/final/training_args.bin` - 训练参数

---

## 阶段二：模型转换与量化

### 步骤1：合并LoRA权重

#### 1.1 创建合并脚本

如果 `stage2_quantize/merge_lora.py` 不存在，需要创建：

```bash
# 检查文件是否存在
ls stage2_quantize/merge_lora.py
```

#### 1.2 运行合并脚本

```bash
python stage2_quantize/merge_lora.py \
    --base_model Qwen/Qwen2.5-0.5B-Instruct \
    --lora_path outputs/checkpoints/final \
    --output_path outputs/merged_model
```

**合并过程：**
- 加载基础模型和LoRA权重
- 将LoRA权重合并到基础模型中
- 保存为完整的HuggingFace格式模型

**输出：**
- `outputs/merged_model/` - 合并后的完整模型（约1GB，FP16格式）
  - `config.json` - 模型配置
  - `pytorch_model.bin` 或 `model.safetensors` - 模型权重
  - `tokenizer.json` - Tokenizer文件

---

### 步骤2：安装 llama.cpp

#### 2.1 获取llama.cpp仓库

**方法1: 手动下载（推荐，适合网络慢的情况）**

```bash
# 如果已经手动下载了llama.cpp-master.zip并解压
cd /home/ljh/AI

# 重命名为llama.cpp（如果还是llama.cpp-master）
if [ -d "llama.cpp-master" ]; then
    mv llama.cpp-master llama.cpp
fi

# 运行配置脚本检查
bash setup_llama_cpp.sh

cd llama.cpp
```

**方法2: 使用Git克隆**

```bash
# 使用GitHub（如果网络允许）
cd /home/ljh/AI
git clone https://github.com/ggerganov/llama.cpp.git

# 或使用Gitee镜像（推荐，适合中国大陆）
git clone https://gitee.com/mirrors/llama.cpp.git

cd llama.cpp
```

**验证仓库：**
```bash
# 检查关键文件
ls -la Makefile convert_hf_to_gguf.py requirements.txt
```

#### 2.2 编译（Linux/WSL2）

**注意：** 新版本的llama.cpp使用CMake构建系统，不再使用Makefile。

**方法1: 使用一键编译脚本（推荐，默认GPU版本）**

```bash
cd /home/ljh/AI/llama.cpp

# GPU版本（默认，需要CUDA）
bash 编译GPU版本.sh

# 或使用智能编译脚本（自动检测，默认尝试GPU）
bash 一键编译.sh

# 或交互式选择
bash 编译llama.cpp.sh
```

**方法2: 手动编译**

```bash
# 安装编译依赖
sudo apt update
sudo apt install -y build-essential cmake

# 创建build目录
cd /home/ljh/AI/llama.cpp
mkdir -p build
cd build

# 配置CMake（GPU版本，默认，需要CUDA）
cmake .. -DCMAKE_BUILD_TYPE=Release -DGGML_CUDA=ON -DLLAMA_CURL=OFF
# 注意: 新版本使用 GGML_CUDA 替代已弃用的 LLAMA_CUBLAS
# 注意: -DLLAMA_CURL=OFF 禁用CURL功能（如果未安装libcurl）

# 或配置CMake（CPU版本，不推荐）
# cmake .. -DCMAKE_BUILD_TYPE=Release -DLLAMA_CURL=OFF

# 编译
cmake --build . --config Release -j$(nproc)

# 编译完成后，可执行文件在 build/bin/ 目录下：
# - llama-cli (推理工具)
# - llama-quantize (量化工具，注意名称包含llama-前缀)
```

**验证编译：**
```bash
# 检查可执行文件
ls -lh build/bin/llama-cli build/bin/llama-quantize

# 查看帮助
./build/bin/llama-cli --help
./build/bin/llama-quantize --help
```

**快捷方式（可选）：**
```bash
# 创建符号链接到llama.cpp根目录，方便使用
cd /home/ljh/AI/llama.cpp
ln -sf build/bin/llama-cli llama-cli
ln -sf build/bin/llama-quantize quantize
# 注意: 实际文件名是 llama-quantize，创建为 quantize 方便使用
```

#### 2.3 安装Python依赖（用于转换脚本）

```bash
# 在llama.cpp目录下
pip install -r requirements.txt -i https://pypi.tuna.tsinghua.edu.cn/simple

# 可能需要安装额外的依赖
pip install numpy -i https://pypi.tuna.tsinghua.edu.cn/simple
```

---

### 步骤3：转换模型为GGUF格式

#### 3.1 运行转换脚本

```bash
# 确保在llama.cpp目录下
cd /home/ljh/AI/llama.cpp

# 运行转换脚本（注意：脚本名是 convert_hf_to_gguf.py，使用下划线）
# 注意：使用 --outfile 指定输出文件路径（完整路径），不是 --outdir
# 先创建输出目录
mkdir -p ../outputs/gguf

# 运行转换（需要激活虚拟环境）
python convert_hf_to_gguf.py \
    ../outputs/merged_model \
    --outfile ../outputs/gguf/qwen2.5-0.5b-instruct-f16.gguf \
    --outtype f16
```

**参数说明：**
- `../outputs/merged_model` - 合并后的模型路径（相对于llama.cpp目录）
- `--outfile` - 输出文件路径（完整路径，包括文件名）
- `--outtype f16` - 输出为FP16格式（后续再量化）

**注意：**
- 脚本名称是 `convert_hf_to_gguf.py`（使用下划线，不是横线）
- 如果找不到脚本，运行 `bash setup_llama_cpp.sh` 检查

**输出：**
- `outputs/gguf/qwen2.5-0.5b-instruct-f16.gguf` (约1GB)

#### 3.2 验证转换结果

```bash
# 检查文件是否存在
ls -lh ../outputs/gguf/*.gguf

# 查看文件信息
file ../outputs/gguf/*.gguf
```

---

### 步骤4：量化模型

#### 4.1 运行量化工具

```bash
# 在llama.cpp目录下
# 如果创建了符号链接，直接使用：
./quantize \
    ../outputs/gguf/qwen2.5-0.5b-instruct-f16.gguf \
    ../outputs/gguf/qwen2.5-0.5b-instruct-q4_k_m.gguf \
    Q4_K_M

# 或者使用完整路径（注意：实际文件名是 llama-quantize）：
./build/bin/llama-quantize \
    ../outputs/gguf/qwen2.5-0.5b-instruct-f16.gguf \
    ../outputs/gguf/qwen2.5-0.5b-instruct-q4_k_m.gguf \
    Q4_K_M
```

**量化级别说明：**
- `Q4_0` - 4-bit量化，最快，文件最小
- `Q4_K_M` - 4-bit量化，中等质量（推荐，平衡质量和大小）
- `Q4_K_S` - 4-bit量化，较小文件
- `Q5_0` - 5-bit量化，更好质量
- `Q5_K_M` - 5-bit量化，中等质量
- `Q8_0` - 8-bit量化，接近FP16质量

**输出：**
- `outputs/gguf/qwen2.5-0.5b-instruct-q4_k_m.gguf` (约300MB)

**文件大小对比：**
- 原始模型（FP16）: ~1GB
- 量化后（Q4_K_M）: ~300MB
- 压缩比: 约3.3倍

---

### 步骤5：验证量化模型

#### 5.1 测试推理

```bash
# 在llama.cpp目录下
# 如果创建了符号链接，直接使用：
./llama-cli \
    -m ../outputs/gguf/qwen2.5-0.5b-instruct-q4_k_m.gguf \
    -p "请分析以下文本的情感倾向（正面/负面）：这部电影很好看" \
    -n 50 \
    --temp 0.7

# 或者使用完整路径：
./build/bin/llama-cli \
    -m ../outputs/gguf/qwen2.5-0.5b-instruct-q4_k_m.gguf \
    -p "请分析以下文本的情感倾向（正面/负面）：这部电影很好看" \
    -n 50 \
    --temp 0.7
```

**参数说明：**
- `-m` - 模型路径
- `-p` - 提示文本
- `-n` - 生成token数量
- `--temp` - 温度参数（控制随机性）

#### 5.2 批量测试（推荐）

使用批量评估脚本进行完整的模型评估：

```bash
# 激活虚拟环境（如果还没有）
cd /home/ljh/AI
source venv/bin/activate

# 测试单个量化模型（测试全部样本）
python stage2_quantize/test_quantized_model.py \
    --model_path outputs/gguf/qwen2.5-0.5b-instruct-q4_k_m.gguf

# 测试单个模型（限制样本数，快速测试）
python stage2_quantize/test_quantized_model.py \
    --model_path outputs/gguf/qwen2.5-0.5b-instruct-q4_k_m.gguf \
    --max_samples 100

# 对比多个量化模型
python stage2_quantize/test_quantized_model.py \
    --model_paths \
        outputs/gguf/qwen2.5-0.5b-instruct-f16.gguf \
        outputs/gguf/qwen2.5-0.5b-instruct-q4_k_m.gguf \
        outputs/gguf/qwen2.5-0.5b-instruct-q8_0.gguf \
    --max_samples 200

# 使用GPU加速（如果编译了GPU版本）
python stage2_quantize/test_quantized_model.py \
    --model_path outputs/gguf/qwen2.5-0.5b-instruct-q4_k_m.gguf \
    --gpu_layers 20 \
    --max_samples 100

# 保存详细结果到JSON文件
python stage2_quantize/test_quantized_model.py \
    --model_path outputs/gguf/qwen2.5-0.5b-instruct-q4_k_m.gguf \
    --max_samples 100 \
    --output evaluation_results.json
```

**评估指标说明：**
- **准确率 (Accuracy)**: 整体预测正确的比例
- **精确率 (Precision)**: 预测为正面的样本中，真正为正面的比例
- **召回率 (Recall)**: 所有真正的正面样本中，被正确预测的比例
- **F1分数**: 精确率和召回率的调和平均
- **混淆矩阵**: TP, FP, TN, FN 的详细统计
- **推理时间**: 平均每个样本的推理耗时

**输出示例：**
```
评估报告
============================================================
模型路径: outputs/gguf/qwen2.5-0.5b-instruct-q4_k_m.gguf
测试样本数: 100
正确数: 85

整体指标:
  准确率 (Accuracy): 0.8500 (85.00%)

正面类别指标:
  精确率 (Precision): 0.8600
  召回率 (Recall): 0.8800
  F1分数: 0.8699

负面类别指标:
  精确率 (Precision): 0.8400
  召回率 (Recall): 0.8200
  F1分数: 0.8299

混淆矩阵:
  真正例 (TP): 44
  假正例 (FP): 7
  真负例 (TN): 41
  假负例 (FN): 8

性能指标:
  平均推理时间: 0.123秒/样本
  总耗时: 12.30秒
============================================================
```

---

## 常见问题与解决方案

### Q1: 数据集下载失败

**解决方案：**
1. 检查网络连接
2. 确认 `HF_ENDPOINT` 环境变量已设置
3. 尝试使用ModelScope：`pip install modelscope && python -c "from modelscope import MsDataset; ds = MsDataset.load('seamew/ChnSentiCorp')"`
4. 使用脚本提供的示例数据（用于测试）
5. 手动从Gitee下载：https://gitee.com/mirrors/ChineseNlpCorpus

### Q2: bitsandbytes 安装失败

**解决方案：**
1. 在WSL2中，可能需要安装额外的依赖：
   ```bash
   sudo apt install -y build-essential
   pip install bitsandbytes -i https://pypi.tuna.tsinghua.edu.cn/simple
   ```

2. 如果仍然失败，修改 `config/training_config.yaml`：
   ```yaml
   model:
     use_4bit: false  # 禁用4-bit量化，使用标准LoRA
   ```
   这样会需要更多显存，但功能相同。

### Q3: 显存不足（OOM）

**解决方案：**
1. 减小批大小：`per_device_train_batch_size: 1`
2. 增加梯度累积：`gradient_accumulation_steps: 8`
3. 减小LoRA秩：`r: 4`（在config中）
4. 启用梯度检查点：`gradient_checkpointing: true`（已默认启用）
5. 使用4-bit量化：`use_4bit: true`（已默认启用）

### Q4: 模型下载失败

**解决方案：**
1. 确认 `HF_ENDPOINT=https://hf-mirror.com` 已设置
2. 手动从镜像站下载：
   ```bash
   # 访问 https://hf-mirror.com/Qwen/Qwen2.5-0.5B-Instruct
   # 下载所有文件到 ./models/cache/Qwen/Qwen2.5-0.5B-Instruct/
   ```

### Q5: llama.cpp 编译失败

**解决方案：**
1. 确保安装了编译工具：
   ```bash
   sudo apt update
   sudo apt install -y build-essential cmake
   ```

2. 如果CUDA编译失败，使用CPU版本：
   ```bash
   make clean
   make
   ```

3. 查看编译错误信息，可能需要安装额外的库

### Q6: convert-hf-to-gguf.py 找不到

**解决方案：**
1. 确认在 llama.cpp 目录下运行
2. 检查脚本名称，可能是：
   - `convert-hf-to-gguf.py`
   - `convert.py`
   - `convert_hf_to_gguf.py`
3. 查看 llama.cpp 最新文档确认正确的脚本名

### Q7: 量化后模型性能下降

**解决方案：**
1. 尝试使用更高质量的量化级别：`Q5_K_M` 或 `Q8_0`
2. 检查原始模型微调质量
3. 某些任务对量化更敏感，可能需要调整量化策略

---

## 项目文件结构

```
/home/ljh/AI/
├── README.md                      # 项目说明
├── requirements.txt               # Python依赖
├── 完整执行指南.md                # 本文档
├── config/
│   └── training_config.yaml      # 训练配置
├── data/
│   ├── chn_senti_corp/           # 原始数据集
│   ├── processed/                # 预处理后的数据
│   └── cache/                    # 数据集缓存
├── models/
│   └── cache/                    # 模型缓存
├── stage1_finetune/
│   ├── download_data.py          # 数据下载脚本
│   ├── preprocess_data.py        # 数据预处理脚本
│   ├── train_lora.py             # LoRA训练脚本
│   └── evaluate.py               # 模型评估脚本
├── stage2_quantize/
│   ├── merge_lora.py             # LoRA权重合并脚本
│   └── test_quantized_model.py   # 量化模型测试脚本
├── outputs/
│   ├── checkpoints/              # 训练检查点
│   │   └── final/                # 最终模型（LoRA权重）
│   ├── merged_model/             # 合并后的完整模型
│   └── gguf/                     # GGUF格式模型
└── llama.cpp/                    # llama.cpp工具链（需单独克隆）
```

---

## 下一步：移动端部署

量化后的GGUF模型可以：

1. **使用 llama.cpp 在Android/iOS上编译运行**
   - 参考 llama.cpp 的移动端编译文档
   - 将模型文件部署到移动设备

2. **使用 llama-cpp-python 在Python应用中使用**
   ```bash
   pip install llama-cpp-python
   ```

3. **转换为其他移动端框架格式**
   - ONNX格式（用于移动端推理）
   - CoreML格式（用于iOS）
   - TensorFlow Lite格式

---

## 参考资源

- [Qwen2.5 官方文档](https://qwen.readthedocs.io/)
- [llama.cpp 官方仓库](https://github.com/ggerganov/llama.cpp)
- [PEFT 文档](https://huggingface.co/docs/peft)
- [HuggingFace镜像站](https://hf-mirror.com)
- [ModelScope](https://www.modelscope.cn)

---

## 执行检查清单

### 阶段一检查清单

- [ ] 环境配置完成（虚拟环境、依赖包、HF镜像）
- [ ] GPU环境验证通过
- [ ] 数据集下载成功
- [ ] 数据预处理完成
- [ ] 训练配置检查完成
- [ ] 模型训练完成
- [ ] 模型评估完成

### 阶段二检查清单

- [ ] LoRA权重合并完成
- [ ] llama.cpp 编译成功
- [ ] 模型转换为GGUF格式
- [ ] 模型量化完成
- [ ] 量化模型验证通过

---

**祝您顺利完成课程设计！如有问题，请参考常见问题部分或查看相关文档。**


