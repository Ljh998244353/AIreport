# 执行步骤总结（快速参考）

## 前置准备

```bash
# 1. 进入项目目录
cd /home/ljh/AI

# 2. 激活虚拟环境
source venv/bin/activate

# 3. 设置HuggingFace镜像
export HF_ENDPOINT=https://hf-mirror.com

# 4. 运行环境检查
python check_environment.py
```

---

## 阶段一：模型微调（约30-60分钟）

### 步骤1: 下载数据集
```bash
python stage1_finetune/download_data.py
```
**说明：** 脚本会自动尝试多种下载方案，如果都失败会创建示例数据

### 步骤2: 预处理数据
```bash
python stage1_finetune/preprocess_data.py
```

### 步骤3: 开始训练
```bash
python stage1_finetune/train_lora.py --config config/training_config.yaml
```

### 步骤4: 评估模型
```bash
python stage1_finetune/evaluate.py --model_path outputs/checkpoints/final
```

---

## 阶段二：模型转换与量化（约10-20分钟）

### 步骤1: 合并LoRA权重
```bash
python stage2_quantize/merge_lora.py \
    --base_model Qwen/Qwen2.5-0.5B-Instruct \
    --lora_path outputs/checkpoints/final \
    --output_path outputs/merged_model
```

### 步骤2: 安装llama.cpp
```bash
# 克隆仓库
git clone https://gitee.com/mirrors/llama.cpp.git
cd llama.cpp

# 编译
make

# 安装Python依赖
pip install -r requirements.txt -i https://pypi.tuna.tsinghua.edu.cn/simple
```

### 步骤3: 转换为GGUF格式
```bash
# 在llama.cpp目录下
python convert-hf-to-gguf.py \
    ../outputs/merged_model \
    --outfile ../outputs/gguf/qwen2.5-0.5b-instruct-f16.gguf \
    --outtype f16
```

### 步骤4: 量化模型
```bash
./quantize \
    ../outputs/gguf/qwen2.5-0.5b-instruct-f16.gguf \
    ../outputs/gguf/qwen2.5-0.5b-instruct-q4_k_m.gguf \
    Q4_K_M

# 或使用完整路径：
# ./build/bin/llama-quantize ...
```

### 步骤5: 测试量化模型
```bash
./llama-cli \
    -m ../outputs/gguf/qwen2.5-0.5b-instruct-q4_k_m.gguf \
    -p "请分析以下文本的情感倾向（正面/负面）：这部电影很好看" \
    -n 50
```

---

## 一键执行（推荐）

```bash
# 运行自动化脚本
bash 快速开始.sh
```

---

## 常见问题快速解决

### 数据集下载失败
```bash
# 使用ModelScope
pip install modelscope -i https://pypi.tuna.tsinghua.edu.cn/simple
python -c "from modelscope import MsDataset; ds = MsDataset.load('seamew/ChnSentiCorp')"
```

### 显存不足
编辑 `config/training_config.yaml`:
- `per_device_train_batch_size: 1`
- `gradient_accumulation_steps: 8`
- `r: 4` (LoRA秩)

### bitsandbytes安装失败
编辑 `config/training_config.yaml`:
- `use_4bit: false`

---

## 输出文件位置

- **LoRA权重**: `outputs/checkpoints/final/`
- **合并模型**: `outputs/merged_model/`
- **GGUF模型**: `outputs/gguf/qwen2.5-0.5b-instruct-q4_k_m.gguf` (约300MB)

---

详细说明请参考：`完整执行指南.md`


